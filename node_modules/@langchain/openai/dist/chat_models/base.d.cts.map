{"version":3,"file":"base.d.cts","names":["____________langchain_core_dist_utils_stream_js0","ClientOptions","OpenAI","OpenAIClient","AIMessageChunk","BaseMessage","ChatGeneration","BaseChatModel","LangSmithParams","BaseChatModelParams","BaseChatModelCallOptions","BaseFunctionCallOptions","BaseLanguageModelInput","StructuredOutputMethodOptions","ModelProfile","Runnable","InteropZodType","OpenAICallOptions","OpenAIChatInput","OpenAICoreRequestOptions","ChatOpenAIResponseFormat","ResponseFormatConfiguration","OpenAIVerbosityParam","OpenAIToolChoice","ChatOpenAIToolType","ResponsesToolChoice","OpenAILLMOutput","BaseChatOpenAICallOptions","Chat","ChatCompletionStreamOptions","ChatCompletionModality","Array","ChatCompletionAudioParam","ChatCompletionPredictionContent","Reasoning","ChatCompletionCreateParams","BaseChatOpenAIFields","Partial","BaseChatOpenAI","CallOptions","Record","Omit","ChatCompletionTool","_langchain_core_messages0","MessageStructure","IterableReadableStream","Promise","Function","ChatCompletionFunctionCallOption","RunOutput"],"sources":["../../src/chat_models/base.d.ts"],"sourcesContent":["import { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { type ChatGeneration } from \"@langchain/core/outputs\";\nimport { BaseChatModel, type LangSmithParams, type BaseChatModelParams, BaseChatModelCallOptions } from \"@langchain/core/language_models/chat_models\";\nimport { type BaseFunctionCallOptions, type BaseLanguageModelInput, type StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport { ModelProfile } from \"@langchain/core/language_models/profile\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { InteropZodType } from \"@langchain/core/utils/types\";\nimport { type OpenAICallOptions, type OpenAIChatInput, type OpenAICoreRequestOptions, type ChatOpenAIResponseFormat, ResponseFormatConfiguration, OpenAIVerbosityParam } from \"../types.js\";\nimport { OpenAIToolChoice, ChatOpenAIToolType, ResponsesToolChoice } from \"../utils/tools.js\";\ninterface OpenAILLMOutput {\n    tokenUsage: {\n        completionTokens?: number;\n        promptTokens?: number;\n        totalTokens?: number;\n    };\n}\nexport type { OpenAICallOptions, OpenAIChatInput };\nexport interface BaseChatOpenAICallOptions extends BaseChatModelCallOptions, BaseFunctionCallOptions {\n    /**\n     * Additional options to pass to the underlying axios request.\n     */\n    options?: OpenAICoreRequestOptions;\n    /**\n     * A list of tools that the model may use to generate responses.\n     * Each tool can be a function, a built-in tool, or a custom tool definition.\n     * If not provided, the model will not use any tools.\n     */\n    tools?: ChatOpenAIToolType[];\n    /**\n     * Specifies which tool the model should use to respond.\n     * Can be an {@link OpenAIToolChoice} or a {@link ResponsesToolChoice}.\n     * If not set, the model will decide which tool to use automatically.\n     */\n    // TODO: break OpenAIToolChoice and ResponsesToolChoice into options sub classes\n    tool_choice?: OpenAIToolChoice | ResponsesToolChoice;\n    /**\n     * Adds a prompt index to prompts passed to the model to track\n     * what prompt is being used for a given generation.\n     */\n    promptIndex?: number;\n    /**\n     * An object specifying the format that the model must output.\n     */\n    response_format?: ChatOpenAIResponseFormat;\n    /**\n     * When provided, the completions API will make a best effort to sample\n     * deterministically, such that repeated requests with the same `seed`\n     * and parameters should return the same result.\n     */\n    seed?: number;\n    /**\n     * Additional options to pass to streamed completions.\n     * If provided, this takes precedence over \"streamUsage\" set at\n     * initialization time.\n     */\n    stream_options?: OpenAIClient.Chat.ChatCompletionStreamOptions;\n    /**\n     * The model may choose to call multiple functions in a single turn. You can\n     * set parallel_tool_calls to false which ensures only one tool is called at most.\n     * [Learn more](https://platform.openai.com/docs/guides/function-calling#parallel-function-calling)\n     */\n    parallel_tool_calls?: boolean;\n    /**\n     * If `true`, model output is guaranteed to exactly match the JSON Schema\n     * provided in the tool definition. If `true`, the input schema will also be\n     * validated according to\n     * https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n     *\n     * If `false`, input schema will not be validated and model output will not\n     * be validated.\n     *\n     * If `undefined`, `strict` argument will not be passed to the model.\n     */\n    strict?: boolean;\n    /**\n     * Output types that you would like the model to generate for this request. Most\n     * models are capable of generating text, which is the default:\n     *\n     * `[\"text\"]`\n     *\n     * The `gpt-4o-audio-preview` model can also be used to\n     * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n     * this model generate both text and audio responses, you can use:\n     *\n     * `[\"text\", \"audio\"]`\n     */\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    /**\n     * Parameters for audio output. Required when audio output is requested with\n     * `modalities: [\"audio\"]`.\n     * [Learn more](https://platform.openai.com/docs/guides/audio).\n     */\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    /**\n     * Static predicted output content, such as the content of a text file that is being regenerated.\n     * [Learn more](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs).\n     */\n    prediction?: OpenAIClient.ChatCompletionPredictionContent;\n    /**\n     * Options for reasoning models.\n     *\n     * Note that some options, like reasoning summaries, are only available when using the responses\n     * API. If these options are set, the responses API will be used to fulfill the request.\n     *\n     * These options will be ignored when not using a reasoning model.\n     */\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\"\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates. Replaces the `user` field.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey?: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n}\nexport interface BaseChatOpenAIFields extends Partial<OpenAIChatInput>, BaseChatModelParams {\n    /**\n     * Optional configuration options for the OpenAI client.\n     */\n    configuration?: ClientOptions;\n}\n/** @internal */\nexport declare abstract class BaseChatOpenAI<CallOptions extends BaseChatOpenAICallOptions> extends BaseChatModel<CallOptions, AIMessageChunk> implements Partial<OpenAIChatInput> {\n    temperature?: number;\n    topP?: number;\n    frequencyPenalty?: number;\n    presencePenalty?: number;\n    n?: number;\n    logitBias?: Record<string, number>;\n    model: string;\n    modelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n    stop?: string[];\n    stopSequences?: string[];\n    user?: string;\n    timeout?: number;\n    streaming: boolean;\n    streamUsage: boolean;\n    maxTokens?: number;\n    logprobs?: boolean;\n    topLogprobs?: number;\n    apiKey?: string;\n    organization?: string;\n    __includeRawResponse?: boolean;\n    /** @internal */\n    client: OpenAIClient;\n    /** @internal */\n    clientConfig: ClientOptions;\n    /**\n     * Whether the model supports the `strict` argument when passing in tools.\n     * If `undefined` the `strict` argument will not be passed to OpenAI.\n     */\n    supportsStrictToolCalling?: boolean;\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable\n     * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your\n     * OpenAI organization or project. This must be configured directly with OpenAI.\n     *\n     * See:\n     * https://platform.openai.com/docs/guides/your-data\n     * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n     *\n     * @default false\n     */\n    zdrEnabled?: boolean | undefined;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n    protected defaultOptions: CallOptions;\n    _llmType(): string;\n    static lc_name(): string;\n    get callKeys(): string[];\n    lc_serializable: boolean;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_aliases(): Record<string, string>;\n    get lc_serializable_keys(): string[];\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    /** @ignore */\n    _identifyingParams(): Omit<OpenAIClient.Chat.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams(): Omit<OpenAIClient.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    constructor(fields?: BaseChatOpenAIFields);\n    /**\n     * Returns backwards compatible reasoning parameters from constructor params and call options\n     * @internal\n     */\n    protected _getReasoningParams(options?: this[\"ParsedCallOptions\"]): OpenAIClient.Reasoning | undefined;\n    /**\n     * Returns an openai compatible response format from a set of options\n     * @internal\n     */\n    protected _getResponseFormat(resFormat?: CallOptions[\"response_format\"]): ResponseFormatConfiguration | undefined;\n    protected _combineCallOptions(additionalOptions?: this[\"ParsedCallOptions\"]): this[\"ParsedCallOptions\"];\n    /** @internal */\n    _getClientOptions(options: OpenAICoreRequestOptions | undefined): OpenAICoreRequestOptions;\n    // TODO: move to completions class\n    protected _convertChatOpenAIToolToCompletionsTool(tool: ChatOpenAIToolType, fields?: {\n        strict?: boolean;\n    }): OpenAIClient.ChatCompletionTool;\n    bindTools(tools: ChatOpenAIToolType[], kwargs?: Partial<CallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions>;\n    stream(input: BaseLanguageModelInput, options?: CallOptions): Promise<import(\"../../../../langchain-core/dist/utils/stream.js\").IterableReadableStream<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>>;\n    invoke(input: BaseLanguageModelInput, options?: CallOptions): Promise<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>;\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput;\n    getNumTokensFromMessages(messages: BaseMessage[]): Promise<{\n        totalCount: number;\n        countPerMessage: number[];\n    }>;\n    /** @internal */\n    protected _getNumTokensFromGenerations(generations: ChatGeneration[]): Promise<number>;\n    /** @internal */\n    protected _getEstimatedTokenCountFromPrompt(messages: BaseMessage[], functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[], function_call?: \"none\" | \"auto\" | OpenAIClient.Chat.ChatCompletionFunctionCallOption): Promise<number>;\n    /**\n     * Return profiling information for the model.\n     *\n     * Provides information about the model's capabilities and constraints,\n     * including token limits, multimodal support, and advanced features like\n     * tool calling and structured output.\n     *\n     * @returns {ModelProfile} An object describing the model's capabilities and constraints\n     *\n     * @example\n     * ```typescript\n     * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n     * const profile = model.profile;\n     * console.log(profile.maxInputTokens); // 128000\n     * console.log(profile.imageInputs); // true\n     * ```\n     */\n    get profile(): ModelProfile;\n    /** @internal */\n    protected _getStructuredOutputMethod(config: StructuredOutputMethodOptions<boolean>): string | undefined;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<boolean>): Runnable<BaseLanguageModelInput, RunOutput> | Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n}\n"],"mappings":";;;;;;;;;;;;;;UAUU0B,eAAAA;;;;IAAAA,WAAAA,CAAAA,EAAe,MAAA;EAQRC,CAAAA;;AAULH,UAVKG,yBAAAA,SAAkCjB,wBAUvCc,EAViEb,uBAUjEa,CAAAA;EAAkB;;;EAgBgB,OAYzBrB,CAAAA,EAlCPgB,wBAkCyBU;EAA2B;;;;;EAmD5B,KAKnB1B,CAAAA,EApFPqB,kBAoFyBW,EAAAA;EAA0B;;;AA9FqC;AA0GpG;EAAqC;EAAA,WAAiBjB,CAAAA,EAzFpCK,gBAyFoCL,GAzFjBO,mBAyFiBP;EAAe;;;AAAsB;EAO7DoB,WAAAA,CAAAA,EAAAA,MAAc;EAAA;;;EAAiF,eAAElC,CAAAA,EAvFzGgB,wBAuFyGhB;EAAc;;;;;EAwB9G,IAMnBD,CAAAA,EAAAA,MAAayB;EAA6B;;;;;EA6BlB,cACNW,CAAAA,EAvITpC,QAAAA,CAAayB,IAAAA,CAAKC,2BAuITU;EAAW;;;;;EAcpB,mBAIsBJ,CAAAA,EAAAA,OAAAA;EAA0B;;;;;;;;;;;EAqB9B,MAAqBI,CAAAA,EAAAA,OAAAA;EAAW;;;;;;;;;;;;EAE/B,UAAYA,CAAAA,EAjJnCR,KAiJmCQ,CAjJ7BpC,QAAAA,CAAayB,IAAAA,CAAKE,sBAiJWS,CAAAA;EAAW;;;;;EAES,KACjClC,CAAAA,EA9I3BF,QAAAA,CAAayB,IAAAA,CAAKI,wBA8IS3B;EAAW;;;;EAOmB,UAAgBF,CAAAA,EAhJpEA,QAAAA,CAAa8B,+BAgJoGc;EAAQ;;;;;;;;EAuB7C,SAEtFP,CAAAA,EAhKSrC,QAAAA,CAAa+B,SAgKtBM;EAAM;;;;EAAuE,YAG9DA,CAAAA,EA9JHrC,QAAAA,CAAayB,IAAAA,CAAKO,0BA8JfK,CAAAA,cAAAA,CAAAA;EAAM;;;;;EAEsC,cAAkB5B,CAAAA,EAAAA,MAAAA;EAAsB;;;EAAvB,SAM7D4B,CAAAA,EA5JNlB,oBA4JMkB;;AAAwES,UA1J7Eb,oBAAAA,SAA6BC,OA0JgDY,CA1JxC/B,eA0JwC+B,CAAAA,EA1JtBxC,mBA0JsBwC,CAAAA;EAAS;;;EAErC,aAAqBrC,CAAAA,EAxJnEX,aAwJmEW;;;AAA8CA,uBArJvG0B,cAqJuG1B,CAAAA,oBArJpEe,yBAqJoEf,CAAAA,SArJjCL,aAqJiCK,CArJnB2B,WAqJmB3B,EArJNR,cAqJMQ,CAAAA,YArJqByB,OAqJrBzB,CArJ6BM,eAqJ7BN,CAAAA,CAAAA;EAAsB,WAC9IP,CAAAA,EAAAA,MAAAA;EAAW,IACR4C,CAAAA,EAAAA,MAAAA;EAAS,gBAFmGlC,CAAAA,EAAAA,MAAAA;EAAQ,eArJhCR,CAAAA,EAAAA,MAAAA;EAAa,CAAA,CAAA,EAAyC8B,MAAAA;EAAO,SAAA,CAAA,EAMjJG,MANiJ,CAAA,MAAA,EAAA,MAAA,CAAA;;gBAQ/ItB;;;;;;;;;;;;;;UAcNf;;gBAEMF;;;;;;UAMNE,QAAAA,CAAayB,IAAAA,CAAKI;eACbD,MAAM5B,QAAAA,CAAayB,IAAAA,CAAKE;cACzB3B,QAAAA,CAAa+B;;;;;;;;;;;;;;;;;iBAiBV/B,QAAAA,CAAayB,IAAAA,CAAKO;;;;;;;;;;cAUrBb;4BACciB;;;;;;;;oBAQRC;;mDAE+BhC;;wBAE3BiC,KAAKtC,QAAAA,CAAayB,IAAAA,CAAKO;;MAEzClC;;;;uBAIiBwC,KAAKtC,QAAAA,CAAagC;;MAEnClC;uBACiBmC;;;;;sEAK+CjC,QAAAA,CAAa+B;;;;;2CAKxCK,iCAAiClB;;;6BAG/CF,uCAAuCA;;0DAEVK;;MAEpDrB,QAAAA,CAAauC;mBACAlB,+BAA+Ba,QAAQE,eAAexB,SAASH,wBAAwBR,gBAAgBmC;gBAC1G3B,kCAAkC2B,cAAcO,QAAuG,uBAAd1C,eAA5FuC,yBAAAA,CAA8IC,gBAAAA;gBAC3LhC,kCAAkC2B,cAAcO,QAAQ1C,eAAXuC,yBAAAA,CAA6DC,gBAAAA;;mCAEvFlB,oBAAoBA;qCAClBrB,gBAAgByC;;;;;sDAKCxC,mBAAmBwC;;wDAEjBzC,2BAA2BF,QAAAA,CAAayB,IAAAA,CAAKO,0BAAAA,CAA2BY,8CAA8C5C,QAAAA,CAAayB,IAAAA,CAAKoB,mCAAmCF;;;;;;;;;;;;;;;;;;iBAkBlNhC;;+CAE8BD;;;oBAG3B2B,sBAAsBA,mCAAmCxB,eAAeiC;;IAEvFT,8BAA8B3B,uCAAuCE,SAASH,wBAAwBqC;;;oBAGvFT,sBAAsBA,mCAAmCxB,eAAeiC;;IAEvFT,8BAA8B3B,sCAAsCE,SAASH;SACvEP;YACG4C;;;;oBAIMT,sBAAsBA,mCAAmCxB,eAAeiC;;IAEvFT,8BAA8B3B,yCAAyCE,SAASH,wBAAwBqC,aAAalC,SAASH;SACxHP;YACG4C"}